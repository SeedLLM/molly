<div align="center">
<img width="400" height="240" alt="Image" src="https://github.com/user-attachments/assets/94b8192d-7f5b-49ed-b2fa-861c643b8b7a" />
</div>

## Molly

Molly is a Large Language Model composed of multiple encoders, capable of understanding multi-omics data (DNA, RNA, and protein).

Molly æ˜¯ä¸€ä¸ªé›†æˆäº†å¤šä¸ª encoder çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£ DNAï¼ŒRNA å’Œ protein åºåˆ—ä¿¡æ¯ã€‚
<img width="994" height="369" alt="Image" src="https://github.com/user-attachments/assets/65b17c06-0506-40a3-bd25-e59172630cff" />
Omics-Specific Modelsï¼ˆOSMsï¼‰æŒ‡ä»£å„è‡ªç»„å­¦èµ›é“ä¸­æ€§èƒ½é¢†å…ˆçš„ä¸“ç”¨æ¨¡å‹ï¼›Enc-Head åˆ™æ˜¯â€œç»„å­¦ Encoder + åˆ†ç±»å¤´â€çš„ç®€æ´æ¶æ„ï¼Œå°†é¢„è®­ç»ƒç¼–ç å™¨ä¸ä»»åŠ¡ç›¸å…³åˆ†ç±»å¤´ç›´æ¥è¿æ¥ã€‚

## :star2: Feature
- **Base Model**: Enhanced [Qwen3](https://github.com/QwenLM/Qwen3) with [nucleotide-transformer](https://github.com/instadeepai/nucleotide-transformer) and [ESM-2](https://github.com/facebookresearch/esm) encoders
- **Optimization**: Support [Liger-Kernel](https://github.com/linkedin/Liger-Kernel) and [FlashAttention](https://github.com/Dao-AILab/flash-attention) for 100% training speedup, see [example script](./scripts/train/examples/run_train_1B_z2_b1.sh)

## ğŸ¤— Download trained model

<div>
  <tr>
      <td><a href="https://huggingface.co/tpoisonooo/MOLLM-1.7B">molly-1.7B</a></td>
      <td><a href="https://huggingface.co/tpoisonooo/MOLLM-4B">molly-4B</a></td>
      <td><a href="https://huggingface.co/tpoisonooo/MOLLM-8B">molly-8B</a></td>
  </tr>
</div>  

## :zap: How to inference

    ```bash
    ./scripts/infer/inference_nt_lora.sh
    ```

## :fire: How to train

1. Hotfix transformers source code

    ```python
    ## transformers/modeling_utils.py
    ## add 4 lines 
        if not model._tp_plan:
            model_tp_plan = {}
        else:
            model_tp_plan = model._tp_plan
    
    ## model._tp_plan -> model_tp_plan
        tp_plan_regex = (
            re.compile("|".join([re.escape(plan) for plan in model_tp_plan]))
            if _torch_distributed_available and torch.distributed.is_initialized()
            else None
        )
    ```

2. Run training script

    ```bash
    swanlab login
    
    ./scripts/train/run_train.sh
    
    # or for test
    ./scripts/train/run_train_mini.sh
    ```

## :pushpin: LICENSE

This project follows [apache license](./LICENSE).

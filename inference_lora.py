import os
import json
import argparse
from argparse import Namespace
from functools import partial
import re
import traceback
from typing import List, Tuple

import torch
import pandas as pd
from tqdm import tqdm
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
from peft import PeftModel, PeftConfig  # 添加PEFT库导入

# Import our custom modules
from src.model import QwenWithBert, get_qwen_bert_config
from src.utils.tools import print_rank_0


def parse_args():
    parser = argparse.ArgumentParser(description="Multimodal LLM Inference (DNA + Text)")
    parser.add_argument("--local_rank", type=int, default=0,
                        help="Local rank for distributed inference, ignored if single GPU")
    parser.add_argument("--text_model_path", type=str, required=True, 
                        help="Path to text model (Qwen)")
    parser.add_argument("--bio_model_path", type=str, required=True,
                        help="Path to DNA-BERT model")
    parser.add_argument("--trained_model_path", type=str, required=True, 
                        help="Path to trained multimodal model checkpoint")
    parser.add_argument("--use_lora", action="store_true",
                        help="Whether to load LoRA weights (if available)")
    parser.add_argument("--multimodal_k_tokens", type=int, default=128,
                        help="Number of multimodal tokens used for DNA sequence projection (should match training)")
    parser.add_argument("--dataset_path", type=str, required=True,
                        help="Path to dataset file (jsonl/csv/xlsx/json) for batch inference")
    parser.add_argument("--output_path", type=str, default="./results",
                        help="Directory to save batch inference outputs")
    parser.add_argument("--max_length", type=int, default=1024,
                        help="Maximum tokens to generate per example")
    parser.add_argument("--batch_size", type=int, default=8,
                        help="Batch size for dataset inference")
    parser.add_argument("--temperature", type=float, default=0.8)
    parser.add_argument("--top_p", type=float, default=0.95)
    parser.add_argument("--device", type=str, default="cuda", choices=["cpu","cuda"])
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--max_samples", type=int, default=None,
                        help="Maximum number of samples to process (None=all)")
    return parser.parse_args()


class MultiModalInfer:
    """Batch inference helper for Qwen + DNABERT multimodal model (training-style DNA injection)."""

    def __init__(self, args):
        self.args = args

        # ‑- Seed ----------------------------------------------------------------
        torch.manual_seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(args.seed)

        # ‑- Device --------------------------------------------------------------
        self.device = torch.device("cuda" if args.device == "cuda" and torch.cuda.is_available() else "cpu")

        # ‑- Tokenisers ----------------------------------------------------------
        print("Loading tokenizers…")
        self.text_tokenizer = AutoTokenizer.from_pretrained(args.text_model_path, trust_remote_code=True)
        self.dna_tokenizer = AutoTokenizer.from_pretrained(args.bio_model_path, trust_remote_code=True)

        # Ensure DNA special tokens exist in text tokenizer
        special_tokens = ["<|dna_start|>", "<|dna_pad|>", "<|dna_end|>"]
        self.text_tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})

        # ‑- Model --------------------------------------------------------------
        print("Loading multimodal model…")
        self._load_model()

    def _load_model(self):
        args = self.args
        
        # Get model configuration
        model_config = get_qwen_bert_config(args.text_model_path, args.bio_model_path)
        
        # Explicitly set project_token_num to match training
        model_config.project_token_num = args.multimodal_k_tokens
        print(f"Setting project_token_num to {model_config.project_token_num}")
        
        # Create regular training-time model (DNA injected at <|dna_start|>)
        self.model = QwenWithBert(model_config)
        # Important: inform the model of special token IDs so it can find <|dna_start|> etc.
        self.model.set_special_tokens(self.text_tokenizer)
        
        # 检查是否使用LoRA
        if args.use_lora:
            print("LoRA mode enabled, checking for LoRA weights...")
            lora_path = os.path.join(args.trained_model_path, "lora_weights")
            
            if os.path.exists(lora_path):
                print(f"Found LoRA adapter at {lora_path}")
                try:
                    # 加载基础模型
                    print(f"Loading base Qwen model from {args.text_model_path}")
                    qwen_model = AutoModelForCausalLM.from_pretrained(
                        args.text_model_path, 
                        trust_remote_code=True, 
                        torch_dtype=torch.bfloat16
                    )
                    self.model.model.load_state_dict(qwen_model.state_dict())
                    del qwen_model  # 释放内存
                    
                    print(f"Loading base DNA-BERT model from {args.bio_model_path}")
                    dna_model = AutoModel.from_pretrained(
                        args.bio_model_path, 
                        trust_remote_code=True,
                        config=model_config.bio_config
                    )
                    self.model.bio_model.load_state_dict(dna_model.state_dict(), strict=False)
                    del dna_model  # 释放内存
                    
                    # 加载LoRA适配器
                    print(f"Applying LoRA adapter from {lora_path}")
                    config = PeftConfig.from_pretrained(lora_path)
                    self.model.model = PeftModel.from_pretrained(self.model.model, lora_path)
                    
                    # 加载多模态投影器权重（如果存在）
                    projector_path = os.path.join(args.trained_model_path, "pytorch_model.bin")
                    if os.path.exists(projector_path):
                        print(f"Loading projector weights from {projector_path}")
                        try:
                            state_dict = torch.load(projector_path, map_location="cpu")
                            projector_dict = {}
                            for key, value in state_dict.items():
                                if "multimodal_projector" in key:
                                    print(f"Loading projector weights from {key}")
                                    projector_dict[key] = value
                            
                            if projector_dict:
                                missing, unexpected = self.model.load_state_dict(projector_dict, strict=False)
                                print(f"Projector weights loaded. Missing keys: {missing}, Unexpected keys: {unexpected}")
                                print(f"Projector weights loaded. Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}")
                        except Exception as e:
                            print(f"Error loading projector weights: {e}")
                            print(traceback.format_exc())
                except Exception as e:
                    print(f"Error loading LoRA model: {e}")
                    print(traceback.format_exc())
                    print("Falling back to standard model loading...")
                    self._load_standard_model()
            else:
                print(f"LoRA adapter not found at {lora_path}, falling back to standard model...")
                self._load_standard_model()
        else:
            # 标准模型加载
            self._load_standard_model()
            
        # Move model to device and set to evaluation mode
        self.model = self.model.to(torch.bfloat16).to(self.device)
        self.model.eval()
        
    def _load_standard_model(self):
        """加载标准模型（非LoRA版本）"""
        args = self.args
        print(f"Loading standard checkpoint from {args.trained_model_path}")
        
        try:
            # 尝试直接加载权重
            ckpt_path = os.path.join(args.trained_model_path, "pytorch_model.bin")
            if not os.path.exists(ckpt_path):
                ckpt_path = args.trained_model_path
                
            ckpt = torch.load(ckpt_path, map_location="cpu")
            
            # 检查是否为DeepSpeed检查点
            if isinstance(ckpt, dict) and "model_state_dict" in ckpt:
                state_dict = ckpt["model_state_dict"]
            else:
                state_dict = ckpt
                
            # 加载状态字典
            missing, unexpected = self.model.load_state_dict(state_dict, strict=False)
            print(f"Model loaded successfully")
            if missing:
                print(f"Missing keys: {len(missing)} keys")
            if unexpected:
                print(f"Unexpected keys: {len(unexpected)} keys")
        except Exception as e:
            print(f"Error loading checkpoint: {str(e)}")
            print("Loading base models without fine-tuned weights...")
            
            # 加载Qwen模型参数
            print(f"Loading Qwen model from {args.text_model_path}")
            qwen_model = AutoModelForCausalLM.from_pretrained(
                args.text_model_path, 
                trust_remote_code=True, 
                torch_dtype=torch.bfloat16
            )
            self.model.model.load_state_dict(qwen_model.state_dict())
            del qwen_model  # 释放内存
            
            # 加载DNA-BERT模型参数
            print(f"Loading DNA-BERT model from {args.bio_model_path}")
            dna_model = AutoModel.from_pretrained(
                args.bio_model_path, 
                trust_remote_code=True,
                config=self.model.bio_config
            )
            self.model.bio_model.load_state_dict(dna_model.state_dict(), strict=False)
            del dna_model  # 释放内存
            
            print("Base models loaded successfully")

    # ------------------------------------------------------------------
    # Helper: tokenize a DNA string to fixed length k
    # ------------------------------------------------------------------

    def _encode_dna_sequence(self, dna_seq: str) -> torch.LongTensor:
        """Tokenize a DNA sequence with DNABERT tokenizer and pad / truncate to k tokens."""
        k = self.args.multimodal_k_tokens
        ids = self.dna_tokenizer(dna_seq, add_special_tokens=False)["input_ids"]
        if len(ids) < k:
            ids += [self.dna_tokenizer.pad_token_id] * (k - len(ids))
        else:
            ids = ids[:k]
        return torch.LongTensor(ids)

    # ------------------------------------------------------------------
    # Sample processing (mimic training-time _process_text)
    # ------------------------------------------------------------------

    def process_sample(self, raw_text: str):
        """Convert raw text containing <dna>…<dna> tags into model-ready tensors.

        The logic strictly follows `IterableMultimodalDNADataSet._process_text`:
        1.  Extract all DNA sequences in order.
        2.  Build `input_ids` as:  [BOS] + (placeholders for each DNA) + encoded clean text + [EOS]
        3.  Return `dna_ids_list` (list[Tensor]) that aligns with the placeholders.
        """

        pattern = r"<dna>\s*([ACGTacgt]+)\s*<dna>"

        # 1) Extract sequences & clean text
        extracted_dnas: List[str] = re.findall(pattern, raw_text)
        clean_text = re.sub(pattern, "", raw_text).strip()

        # 2) Construct input_ids
        input_ids: List[int] = []

        if self.text_tokenizer.bos_token_id is not None:
            input_ids.append(self.text_tokenizer.bos_token_id)

        # Placeholders for each DNA
        start_id = self.text_tokenizer.convert_tokens_to_ids("<|dna_start|>")
        pad_id = self.text_tokenizer.convert_tokens_to_ids("<|dna_pad|>")
        end_id = self.text_tokenizer.convert_tokens_to_ids("<|dna_end|>")

        dna_ids_list: List[torch.LongTensor] = []
        k = self.args.multimodal_k_tokens

        for dna_seq in extracted_dnas:
            input_ids.append(start_id)
            input_ids.extend([pad_id] * k)
            input_ids.append(end_id)
            dna_ids_list.append(self._encode_dna_sequence(dna_seq))

        # Append (clean) text after all DNA placeholders
        if clean_text:
            input_ids.extend(self.text_tokenizer.encode(clean_text, add_special_tokens=False))

        # Truncate & add EOS
        # if len(input_ids) > self.args.max_length - 1:
        #     input_ids = input_ids[: self.args.max_length - 1]
        # input_ids.append(self.text_tokenizer.eos_token_id)

        attention_mask = [1] * len(input_ids)

        return {
            "input_ids": torch.LongTensor(input_ids),
            "attention_mask": torch.LongTensor(attention_mask),
            "dna_ids_list": dna_ids_list if dna_ids_list else None,
        }

    # ------------------------------------------------------------------
    # Single-sample inference loop (no padding, no batching)
    # ------------------------------------------------------------------

    def run_dataset(self):
        """Run inference sample-by-sample without padding."""
        args = self.args
        
        # Determine file format and read dataset
        ext = args.dataset_path.split('.')[-1].lower()
        reader = {
            'csv': pd.read_csv,
            'xlsx': pd.read_excel,
            'jsonl': partial(pd.read_json, lines=True),
            'json': partial(pd.read_json, lines=False)
        }.get(ext)
        
        if not reader:
            raise ValueError(f"Unsupported file format: {ext}. Supported formats: csv, xlsx, jsonl, json")
        
        print(f"Reading dataset from {args.dataset_path}")
        df = reader(args.dataset_path)
        
        # Limit number of samples if specified
        if args.max_samples:
            df = df.head(args.max_samples)
        
        # Make sure 'input' column exists
        if 'input' not in df.columns:
            raise ValueError("Dataset must contain an 'input' column")
        
        print(f"Processing {len(df)} samples one-by-one…")
        results = []

        for idx, text in tqdm(enumerate(df['input'].tolist()), total=len(df), desc="Infer"):
            sample = self.process_sample(text)

            # Prepare tensors without padding (batch dimension = 1)
            input_ids = sample['input_ids'].unsqueeze(0).to(self.device)
            attention_mask = sample['attention_mask'].unsqueeze(0).to(self.device)
            dna_ids_lists = [sample.get('dna_ids_list', None)]

            try:
                with torch.no_grad():
                    outputs = self.model.generate(
                        input_ids=input_ids,
                        dna_ids_lists=dna_ids_lists,
                        attention_mask=attention_mask,
                        max_length=args.max_length,
                        temperature=args.temperature,
                        top_p=args.top_p,
                        do_sample=True,
                    )

                decoded = self.text_tokenizer.decode(outputs[0], skip_special_tokens=True)
                results.append(decoded)

                if idx <= 10:
                    print("\nSample output:")
                    print(decoded)
                    print("-" * 50)
            except Exception as e:
                print(f"Error during generation: {str(e)}")
                print(traceback.format_exc())

        # Add results to dataframe and save
        result_df = df.head(len(results)).copy()
        result_df['output'] = results
        
        # Create output directory if it doesn't exist
        os.makedirs(args.output_path, exist_ok=True)
        
        # Save to output file
        output_file = os.path.join(args.output_path, f"predictions_{len(results)}_samples.jsonl")
        result_df.to_json(output_file, orient='records', lines=True, force_ascii=False)
        print(f"Saved predictions to {output_file}")


if __name__ == "__main__":
    args = parse_args()
    inferer = MultiModalInfer(args)
    inferer.run_dataset()

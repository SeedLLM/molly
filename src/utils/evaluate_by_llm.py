import json
import random
from pathlib import Path
from collections import defaultdict
from typing import Union, Dict, List

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from tqdm import tqdm

def sample_jsonl_by_task(
    input_dir: Union[str, Path],
    output_path: Union[str, Path],
    sample_num_per_task: int = 1
) -> None:
    """
    ä» input_dir ä¸‹æ‰€æœ‰ .jsonl æ–‡ä»¶ä¸­æŒ‰ task å­—æ®µåˆ†ç»„é‡‡æ ·ï¼Œæ¯ä¸ª task éšæœºæŠ½ sample_num_per_task æ¡å†™å…¥ output_pathã€‚

    å‚æ•°:
        input_dir (str | Path): è¾“å…¥ç›®å½•ï¼ŒåŒ…å«å¤šä¸ª jsonl æ–‡ä»¶
        output_path (str | Path): è¾“å‡º jsonl æ–‡ä»¶è·¯å¾„
        sample_num_per_task (int): æ¯ä¸ª task æŠ½æ ·çš„æ•°é‡ï¼ˆé»˜è®¤ 1 æ¡ï¼‰
    example:
        sample_jsonl_by_task(
        input_dir="xxx",
        output_path="sampled_by_task.jsonl",
        sample_num_per_task=3  # æ¯ä¸ªä»»åŠ¡æŠ½å–3æ¡
    )
    """
    input_dir = Path(input_dir)
    output_path = Path(output_path)
    task_to_items = defaultdict(list)

    for file_path in input_dir.glob("*.jsonl"):
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    obj = json.loads(line)
                    task = obj.get("task", "Unknown")
                    task_to_items[task].append(obj)
                except json.JSONDecodeError:
                    continue

    sampled_data = []
    for task, items in task_to_items.items():
        sampled = random.sample(items, min(sample_num_per_task, len(items)))
        sampled_data.extend(sampled)

    with open(output_path, "w", encoding="utf-8") as out_file:
        for item in sampled_data:
            out_file.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"âœ… æ€»å…±é‡‡æ · {len(sampled_data)} æ¡ï¼Œæ¥è‡ª {len(task_to_items)} ä¸ª taskï¼Œå·²å†™å…¥ï¼š{output_path}")


# åŸºäºllmçš„è¯„ä¼°
def evaluate_label_match_qwen_chat(
    input_file: str,
    model_path: str,
    output_file: str = None,
    max_eval: int = None,
    max_new_tokens: int = 128
):
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()

    results = []
    with open(input_file, "r", encoding="utf-8") as f:
        for idx, line in enumerate(tqdm(f, desc="Evaluating")):
            if max_eval and idx >= max_eval:
                break

            obj = json.loads(line)
            label = obj["label"]
            llm_output = obj["pre_output"]
            raw_input = obj.get("input", "")
            raw_output = obj.get("output", "")

            # === Updated prompt with input ===
            prompt = (
                "You are a professional evaluator. Your task is to determine whether the following LLM output "
                "correctly and explicitly conveys the meaning of the given label for the given question.\n\n"
                f"Question: {raw_input}\n"
                f"Label: {label}\n"
                f"LLM Output: {llm_output}\n\n"
                "Answer strictly with \"Yes\" or \"No\" only. Do not explain."
            )

            messages = [{"role": "user", "content": prompt}]
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=False
            )
            inputs = tokenizer([text], return_tensors="pt").to(model.device)

            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.8,
                    top_p=0.95,
                    pad_token_id=tokenizer.eos_token_id
                )

            output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()

            try:
                think_end = len(output_ids) - output_ids[::-1].index(151668)  # </think>
            except ValueError:
                think_end = 0

            thinking_text = tokenizer.decode(output_ids[:think_end], skip_special_tokens=True).strip()
            final_text = tokenizer.decode(output_ids[think_end:], skip_special_tokens=True).strip()

            answer_clean = final_text.strip().lower()
            correct = answer_clean.startswith("yes")

            result = {
                "task": obj.get("task"),
                "input": raw_input,
                "output": raw_output,
                "label": label,
                "llm_output": llm_output,
                "answer_raw": final_text,
                "thinking": thinking_text,
                "correct": correct
            }
            results.append(result)

    if output_file:
        with open(output_file, "w", encoding="utf-8") as f_out:
            for r in results:
                f_out.write(json.dumps(r, ensure_ascii=False) + "\n")
        print(f"âœ… Results saved to {output_file}")

    if results:
        # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
        acc = sum(r["correct"] for r in results)
        print(f"âœ… Overall Accuracy: {acc}/{len(results)} = {acc / len(results):.2%}")
        
        # æŒ‰ä»»åŠ¡ç»Ÿè®¡å‡†ç¡®ç‡
        task_stats = calculate_accuracy_by_task(results)
        print("\nğŸ“Š å„ä»»åŠ¡å‡†ç¡®ç‡ç»Ÿè®¡:")
        print("-" * 60)
        print(f"{'Task Name':<30} | {'Correct':<8} | {'Total':<8} | {'Accuracy':<10}")
        print("-" * 60)
        
        for task, (correct, total) in sorted(task_stats.items(), key=lambda x: x[1][1], reverse=True):
            accuracy = correct / total if total > 0 else 0
            print(f"{task[:30]:<30} | {correct:<8} | {total:<8} | {accuracy:.2%}")
        
        print("-" * 60)
        
        # ä¿å­˜ä»»åŠ¡ç»Ÿè®¡ç»“æœ
        if output_file:
            stats_output = output_file.replace('.jsonl', '_task_stats.json')
            task_stats_json = {
                task: {"correct": correct, "total": total, "accuracy": correct / total if total > 0 else 0}
                for task, (correct, total) in task_stats.items()
            }
            with open(stats_output, "w", encoding="utf-8") as f_stats:
                json.dump(task_stats_json, f_stats, ensure_ascii=False, indent=2)
            print(f"âœ… Task statistics saved to {stats_output}")

    return results

def calculate_accuracy_by_task(results: List[Dict]) -> Dict[str, tuple]:
    """
    æŒ‰ä»»åŠ¡ç»Ÿè®¡å‡†ç¡®ç‡
    
    Args:
        results: è¯„ä¼°ç»“æœåˆ—è¡¨
        
    Returns:
        Dict[str, tuple]: ä»»åŠ¡åç§°åˆ°(æ­£ç¡®æ•°, æ€»æ•°)çš„æ˜ å°„
    """
    task_stats = defaultdict(lambda: [0, 0])  # [correct_count, total_count]
    
    for result in results:
        task = result.get("task", "Unknown")
        task_stats[task][1] += 1  # æ€»æ•°+1
        if result.get("correct", False):
            task_stats[task][0] += 1  # æ­£ç¡®æ•°+1
    
    # å°†åˆ—è¡¨è½¬æ¢ä¸ºå…ƒç»„
    return {task: tuple(counts) for task, counts in task_stats.items()}


if __name__ == "__main__":
    sample_file_path = "/fs-computility/ai4agr/lijinzhe/res_data_model/biomllm_res/inference_results/Qwen3_4B_NT_sft_exp1_0710/predictions_300_samples_step500.jsonl"
    llm_path="/fs-computility/ai4agr/lijinzhe/basemodel/Qwen3-14B"
    # sample_jsonl_by_task(
    #     input_dir="/fs-computility/ai4agr/lijinzhe/res_data_model/0621_test",
    #     output_path="/fs-computility/ai4agr/lijinzhe/res_data_model/0621_test/sampled_by_task.jsonl",
    #     sample_num_per_task=20
    # )
    evaluate_label_match_qwen_chat(
        input_file=sample_file_path,
        model_path=llm_path,
        output_file="Qwen3_4B_NT_sft_exp1_0710_step500.jsonl",
        max_new_tokens=512)

